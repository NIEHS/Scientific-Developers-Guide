---
title: "Coding Assistants"
date-modified: "2025-01-17"
---

## Introduction

Welcome to the NIEHS Coding Assistants Guide. 

This guide is intended to provide a set of best practices and guidelines for using coding assistants in your development workflow. Coding assistants are tools that help you write code more efficiently and effectively by providing suggestions, auto-completions, and other features that streamline the coding process.

## {{< iconify logos github-icon >}} GitHub Copilot

GitHub Copilot is an AI-powered code completion tool that helps you write code faster and with fewer errors. It is based on the OpenAI Codex model and is available as a plugin for Visual Studio Code and other popular code editors. GitHub Copilot can generate code snippets, complete function calls, and provide context-aware suggestions to improve your coding experience.

## ![](/media/continue-logo.png){height=1em} Continue.dev VSCode Extension

[Continue](https://docs.continue.dev/intro) is an open-source AI code assistant. You can connect any models and any context to build custom autocomplete and chat experiences inside [VS Code](/development/ides.qmd#visual-studio-code) and JetBrains.

### Installation

Install the extension from the Visual Studio Code Extension manager, or from the link on the [Continue](https://docs.continue.dev/) website.

### Configuration

You can add new models to the Continue extension by using the following format:

```yaml
{
  "models": [
    {
      "title": "OpenAI GPT-4o",
      "provider": "openai",
      "model": "azure-gpt-4o",
      "apiKey": "sk_1234",
      "apiBase": "https://litellm.toxpipe.niehs.nih.gov/"
    },
    {
      "title": "Claude 3.5 Sonnet",
      "provider": "openai",
      "model": "claude-3-5-sonnet",
      "apiKey": "sk_1234",
      "apiBase": "https://litellm.toxpipe.niehs.nih.gov/"
    },
    {
      "title": "Ollama",
      "provider": "ollama",
      "model": "AUTODETECT"
    },
    ],
  "tabAutocompleteModel": [
    {
      "title": "Starcoder 2",
      "provider": "ollama",
      "model": "starcoder2:3b"
    },
    {
      "title": "qwen2-0.5b",
      "provider": "ollama",
      "model": "qwen2:0.5b"
    }
  ],
"embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text"
  }
}
```

### Tab Autocomplete Models

Tab autocomplete models require locally-available LLMs. The easiest way is to install [`ollama`](https://ollama.com/download), then install the various local models that you will need. It is recommended to use the installer from ollama, as this installer will start up the ollama server in the background on startup. If you use the installer from a package manager like `apt`, `yum`, or `brew`, there will be additional steps required to get the ollama server to start on startup.

You'll usually want to stick with a smaller model for the speed. StarCoder 2 is a great model, but there are many others.

```{bash}
ollama pull starcoder2:3b
```

### Vector embedding models

Using a model like `nomic-embed-text` is a great way to have vector embeddings of your code and relevant documentation. This model is small and can be run locally on most computers. The easiest way to deploy it is to install `ollama` then run `ollama pull nomic-embed-text`.