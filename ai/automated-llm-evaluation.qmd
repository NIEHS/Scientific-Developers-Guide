---
title: "Automated LLM Evaluation"
date-modified: "2024-07-09"
description: "Running automated large language model (LLM) testing using Promptfoo."
image: /media/promptfoo-logo.svg
---

## Introduction

This page shows how to run automated LLM testing. If you are part of the [NIEHS GitHub](https://github.com/NIEHS) organization, you can see an example of this in the [NIEHS/ToxPipe-Model-Comparisons](https://github.com/NIEHS/ToxPipe-Model-Comparison/tree/main/reports/ecsa-gara-challenge) repository.

## Requirements

- [Promptfoo installed](https://www.promptfoo.dev/docs/installation/)
- `.env` file with the following variables:
  - `OPENAI_BASE_URL`
  - `OPENAI_API_KEY`
  - `OPENAI_API_VERSION`
- `promptfooconfig.yaml`


## Running Tests

To run the tests, execute the following command while in the same directory that the files above are in. If you run this command from a parent directory, you will have to use relative file paths.

```bash
npx promptfoo@latest eval --env-file .env
```

The results of the test will be output into the same directory. This can be changed by modifying the `promptfooconfig.yaml` file or using the `--output` flag when running the command.

